


<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>  天池机器学习 |    white.</title>
  <meta name="description" content="A minimalist theme for hexo.">
  <!-- 标签页图标 -->
  

  <!-- 图标库 -->
  <link href="https://cdn.jsdelivr.net/npm/remixicon@2.2.0/fonts/remixicon.css" rel="stylesheet">
  <!-- 动画库 -->
  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/fushaolei/cdn-white@1.0/css/animate.css"/>
  
  <!-- css文件 -->
  
<link rel="stylesheet" href="/css/white.css">

  <!-- 代码高亮 -->
  
<meta name="generator" content="Hexo 5.3.0"></head>


<body>

<div class="menu-outer">
    <div class="menu-inner">
      <div class="menu-site-name  animate__animated  animate__fadeInUp">
        <a href="/">
          white.
        </a>
        
      </div>
      <div class="menu-group">
        <ul class="menu-ul">
        
          <a href="/" class="nav-link">
            <li class="menu-li  animate__animated  animate__fadeInUp">
              HOME
            </li>
          </a>
        
          <a href="/archives" class="nav-link">
            <li class="menu-li  animate__animated  animate__fadeInUp">
              BLOG
            </li>
          </a>
        
        
        
          <li class="menu-li animate__animated  animate__fadeInUp" id="mobile-menu">
            <i class="ri-menu-line"></i>
          </li>
        
        </ul>

      </div>

    </div>
</div>
<div id="mobile-main" class="animate__animated  animate__fadeIn">
  <div class="mobile-menu-inner">
    <div class="mobile-menu-site-name animate__animated  animate__fadeInUp">
      <a href="/">
        white.
      </a>
    </div>
    <div class="mobile-menu-group" id="mobile-close">
      <i class="ri-close-line"></i>
    </div>

  </div>

  <div class="mobile-menu-div">
  
    <a href="/" class="mobile-nav-link">
      <div class="mobile-menu-child animate__animated  animate__fadeInUp">
        <span>HOME</span>
      </div>
    </a>
  
    <a href="/archives" class="mobile-nav-link">
      <div class="mobile-menu-child animate__animated  animate__fadeInUp">
        <span>BLOG</span>
      </div>
    </a>
  
  
  </div>


</div>

<div class="body-outer">
  <div class="body-inner">
    
<article class="post-inner">
  <div class="post-content-outer">
    <div class="post-intro">
      <div class="post-title animate__animated  animate__fadeInUp">天池机器学习</div>
      <div class="meta-intro animate__animated  animate__fadeInUp">Jan 02 2021</div>
      
    </div>
    <div class="post-content-inner">
      <div class="post-content-inner-space">

      </div>
      <div class="post-content-main animate__animated  animate__fadeInUp">
        <!-- top型目录 -->
        
        <h2 id="MY-amp-PY天池机器学习"><a href="#MY-amp-PY天池机器学习" class="headerlink" title="MY&amp;PY天池机器学习"></a>MY&amp;PY天池机器学习</h2><h3 id="第一天"><a href="#第一天" class="headerlink" title="第一天"></a>第一天</h3><p>额，对于其他人来说可能是直接就开始学内容了，可是我啊，是不是选错博客了，我选Github作为写博客的地方，但是却从未用过。<br><br>没想到用好Github也是一件不容易的事情呢，本来想下载一个模板再写的，可是几经波折后发现不是一两天内能够弄好的事情，能够用好Github这个工具也相当于一个训练营的内容了，嗯，一边学训练营的内容一边搞博客空间好了哈哈哈，前期博客就走简单路线了。 <br><br>TASK 1 的代码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br></pre></td><td class="code"><pre><span class="line">Demo实战</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import seaborn as sns</span><br><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line"></span><br><span class="line">x_fearures &#x3D; np.array([-1, -2],[-2, -1],[-3, -2],[1, 3],[2, 1],[3,2]])</span><br><span class="line">y_label &#x3D; np,array([0, 0, 0, 1 , 1, 1])</span><br><span class="line"></span><br><span class="line">Ir_clf &#x3D; LogistincRegression()</span><br><span class="line"></span><br><span class="line">Ir_clf &#x3D; Ir_clf.fit(x_fearures,y_label)</span><br><span class="line"></span><br><span class="line">print(&#39;the weight of Logistic Regression:&#39;,Ir_clf,coef_)</span><br><span class="line"></span><br><span class="line">print(&#39;the intercept(w0) of Logistic Regression:&#39;,Ir_clf.intercept_)</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.scatter(x_fearures[:,0],x_fearures[:,1],c&#x3D; y_label, s&#x3D;50, cmap&#x3D; &#39;viridis&#39;</span><br><span class="line">plt.title(&#39;Dataset&#39;)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.scatter(x_fearures[:,0],x_fearures[:,1],c&#x3D; y_label,s&#x3D;50,cmap&#x3D;&#39;viridis&#39;)</span><br><span class="line">plt.title(&#39;Dataset&#39;)</span><br><span class="line"></span><br><span class="line">nx,ny &#x3D; 200,100</span><br><span class="line">x_min, x_max &#x3D; plt,xlim()</span><br><span class="line">y_min, y_max &#x3D; plt,ylim()</span><br><span class="line">x_grid, y_grid &#x3D; np.meshgrid(np.linspace(x_xin, x_max, nx),np.linspace(y_min, y_max, ny))</span><br><span class="line"></span><br><span class="line">z_proba &#x3D; Ir_clf.predict_proba(np.c_[x_grid.ravel(), y_grid.ravel()])</span><br><span class="line">z_proba &#x3D; z_proba[:, 1].reshape(x_grid.shape)</span><br><span class="line">plt.contour(x_grid,y_grid, z_proba,[0.5], linewidths &#x3D; 2.,colors &#x3D;&#39;blue&#39;)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line"></span><br><span class="line">x_fearures_new1 &#x3D; np.array([[0, -1]])</span><br><span class="line">plt.scatter(x_fearures_new1[:, 0],x_fearures_new1[:.1]. s&#x3D; 50 ,cmap &#x3D; &#39;vididis&#39;)</span><br><span class="line">plt.annotate(s&#x3D; &#39;New point 1&#39;, xy&#x3D;(0,-1),xytext&#x3D; (-2,0),color &#x3D; &#39;blue&#39;, arrowprops &#x3D; dict(arrowstyle&#x3D; &#39;-|&gt;&#39;,connectionstyle&#x3D; &#39;arc3&#39;,color&#x3D; &#39;red&#39;))</span><br><span class="line"></span><br><span class="line">x_fearures_new2 &#x3D; np.array([[1,2]])</span><br><span class="line">plt.scatter(x_fearures_new2[:,0],x_fearures_new2[:,1],s&#x3D; 50, cmap&#x3D; &#39;viridis&#39;)</span><br><span class="line">plt.annotate(s&#x3D;&#39;New point 2‘，xy&#x3D; (1,2),xytext&#x3D; (-1.5,2.5),color&#x3D; &#39;red&#39;,arrowprops&#x3D;dict(arrowstyle&#x3D; &#39;-|&gt;&#39;,connectionstyle&#x3D; &#39;arc3&#39;,color&#x3D; &#39;red&#39;))</span><br><span class="line"></span><br><span class="line">plt.scatter(x_fearures[:,0],x_fearures[:,1],c&#x3D; y_label,s&#x3D;50,cmap&#x3D;&#39;viridis&#39;)</span><br><span class="line">plt.title(&#39;Dataset&#39;)</span><br><span class="line"></span><br><span class="line">plt.contour(x_grid,y_grid,z_proba, [0.5],linewidths&#x3D; 2.,color&#x3D;&#39;bule&#39;)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">y_label_new1_predict &#x3D; Ir_clf.predict(x_fearures_new1)</span><br><span class="line">y_label_new2_predict &#x3D; Ir_clf.predict(x_fearures_new2)</span><br><span class="line"></span><br><span class="line">print(&#39;The New point 1 predict class:\n&#39;,y_label_new1_predict)</span><br><span class="line">print(&#39;The New point 2 predict class:\n&#39;,y_label_new2_predict)</span><br><span class="line"></span><br><span class="line">y_label_new1_predict_proba &#x3D; Ir_clf.predict_proba(x_fearures_new1)</span><br><span class="line">y_label_new2_predict_proba &#x3D; Ir_clf.predict_proba(x_fearures_new2)</span><br><span class="line"></span><br><span class="line">print(&#39;The New pront 1 predict Probability of each class:\n&#39;, y_label_new1_predict_proba)</span><br><span class="line">print(&#39;The New pront 2 predict Probability of each class:\n&#39;, y_label_new2_predict_proba)</span><br><span class="line"></span><br><span class="line">基于鸢尾花数据集的逻辑回归分类实践 </span><br><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">import matplotlib.pyplot as plt </span><br><span class="line">import seaborn as sns</span><br><span class="line"></span><br><span class="line">from sklearn.datasets import load_iris</span><br><span class="line">data &#x3D; load_iris()</span><br><span class="line">iris_target &#x3D; data.target</span><br><span class="line">iris_features &#x3D; pd.DataFrame(data&#x3D;data.data, columns&#x3D;data.feature_names)</span><br><span class="line"></span><br><span class="line">iris_features.info()</span><br><span class="line">iris_features.head()</span><br><span class="line">iris_features.tail()</span><br><span class="line">iris_target</span><br><span class="line">pd.Series(iris_target).value_counts()</span><br><span class="line">iris_features.describe()</span><br><span class="line">iris_all+ iris_features.copy()</span><br><span class="line">iris_all[&#39;target&#39;] &#x3D; iris_target</span><br><span class="line">sna.pairplot(data&#x3D; iris_all,diag_kind&#x3D;&#39;hist&#39;, hue&#x3D; &#39;target&#39;)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">for col in iris_features.columns:</span><br><span class="line">    sns.boxplot(x&#x3D;&#39;target&#39;, y&#x3D;col, saturation&#x3D;0.5, palette&#x3D;&#39;pastel&#39;. data&#x3D; iris_all)</span><br><span class="line">    plt.title(col)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">from mpl_toolkits.mplot3d import Axes3D</span><br><span class="line"></span><br><span class="line">fig &#x3D; plt.figure(figsize&#x3D; (10,8))</span><br><span class="line">ax &#x3D; fig.add_subplot(111, projection&#x3D;&#39;3d&#39;)</span><br><span class="line"></span><br><span class="line">iris_all_class0 &#x3D; iris_all[iris_all[&#39;target&#39;]&#x3D;&#x3D;0].values</span><br><span class="line">iris_all_class1 &#x3D; iris_all[iris_all[&#39;target&#39;]&#x3D;&#x3D;1].values</span><br><span class="line">iris_all_class2 &#x3D; iris_all[iris_all[&#39;target&#39;]&#x3D;&#x3D;2].values</span><br><span class="line"></span><br><span class="line">ax.scatter(iris_all_class0[:,0], iris_all_class0[:,1], iris_all_class0[:,2],label&#x3D;&#39;setosa&#39;)</span><br><span class="line">ax.scatter(iris_all_class1[:,0], iris_all_class0[:,1], iris_all_class0[:,2],label&#x3D;&#39;versicolor&#39;)</span><br><span class="line">ax.scatter(iris_all_class2[:,0], iris_all_class0[:,1], iris_all_class0[:,2],label&#x3D;&#39;virginica&#39;)</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">利用逻辑回归模型在二分类上进行训练和预测</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line"></span><br><span class="line">iris_features_part &#x3D; iris_features.iloc[:100]</span><br><span class="line">iris_target_part &#x3D; iris_target[:100]</span><br><span class="line"></span><br><span class="line">x_train, x_test, y_train, y_test &#x3D; train_test_split(iris_featurs_part, isis_target_part, test_size &#x3D; 0.2, random_state &#x3D; 2020)</span><br><span class="line"></span><br><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line"></span><br><span class="line">clf &#x3D; LogisticRegression(random_state&#x3D;0 , solver &#x3D; &#39;1bfgs&#39;)</span><br><span class="line">clf.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line">print(&#39;the weight of Logistic Regression:&#39; ,clf.coef_)</span><br><span class="line"></span><br><span class="line">print(&#39;the intercept(w0) of Logistic Regression:&#39;, clf.intercept_)</span><br><span class="line"></span><br><span class="line">train_predict &#x3D; clf.predict(x_train)</span><br><span class="line">test_predict &#x3D; clf.predict(x_test)</span><br><span class="line"></span><br><span class="line">from sklearn import metrics</span><br><span class="line"></span><br><span class="line">print(&#39;The accuracy of the Logistic Regression is:&#39;, metrics.accuracy_score(y_train,train_predict))</span><br><span class="line">print(&#39;The accuracy of the Logistic Regression is:&#39;, metrics.accuracy_score(y_test,test_predict))</span><br><span class="line"></span><br><span class="line">coinfusion_matrix_result &#x3D; metrics. confusion_matrix(test_predict. y_test)</span><br><span class="line">print(&#39;The confusion matrix result:\n&#39;, confusion_matrix_result)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize&#x3D; (8,6))</span><br><span class="line">sns.heatmap(confusion_matrix_ressult, annot&#x3D;True. cmap&#x3D; &#39;Bules&#39;)</span><br><span class="line">plt.xlabel(&#39;Predicted labels&#39;)</span><br><span class="line">plt.ylabel(&#39;True labels&#39;)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">x_train, x_test, y_train, y_test &#x3D; train_test_split(iris_features, iris_target, test_size &#x3D; 0.2, random_state &#x3D; 2020)</span><br><span class="line">clf &#x3D; LogisticRegression(random_state&#x3D;0, solver&#x3D;&#39;1bfgs&#39;)</span><br><span class="line">clf.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line">print(&#39;the weight of Logistic Regression:\n&#39;, clf.coef_)</span><br><span class="line">print(&#39;the intercept(w0) of Logistic Regression:\n&#39;, clf.intercept_)</span><br><span class="line"></span><br><span class="line">train_predict &#x3D; clf.predict(x_train)</span><br><span class="line">test_predict &#x3D; clf.predict(x_test)</span><br><span class="line"></span><br><span class="line">train_predict &#x3D; clf.predict(x_train)</span><br><span class="line">test_predict &#x3D; clf.predict(x_test)</span><br><span class="line"></span><br><span class="line">train_predict_proba &#x3D; clf. predict_proba(x_train)</span><br><span class="line">test_predict_proba &#x3D; clf.predict_proba(x_test)</span><br><span class="line"></span><br><span class="line">print(&#39;the test predict Probability of each class:\n&#39;, test_predict_proba)</span><br><span class="line">print(&#39;The accuracy of the Logistic Regression is:&#39;, metrics.accuracy_score(y_train, train_predict))</span><br><span class="line">print(&#39;The accuracy of the Logistic Regression is:&#39;, metrics.accuracy_score(y_test,test_predict))</span><br><span class="line"></span><br><span class="line">confusion_matrix_result &#x3D; metrics.confusion_matrix(test_predict, y_test)</span><br><span class="line">print(&#39;The confusion matrix result:\n&#39;, confusion_matrix_result)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize&#x3D;(8, 6))</span><br><span class="line">sns.heatmap(confusion_matrix_result, annot&#x3D; True, cmap&#x3D; &#39;Bules&#39;</span><br><span class="line">plt.xlabel(&#39;Predicted labels&#39;)</span><br><span class="line">plt.ylabel(True labels&#39;)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>第二天用了一个小时把第一天没敲的代码敲上了 ，嗯，现在开始进行观察。。。</p>
<h3 id="第二天"><a href="#第二天" class="headerlink" title="第二天"></a>第二天<br></h3><p>逻辑回归是适合于二倍以上的分类方法，传统方法<br><br>突出点：模型简单，模型的可解释性强<br><br>由函数图像的对称点将其结果分为0和1<br></p>
<p>Demo<br><br>库：numpy matplotlib.pyplot seaborn <br><br>逻辑回归模型函数： sklearn.liner_model<br><br>查看输入的x和y值所得出的w0,w的方法  intercept_ , coef_<br></p>
<p>模型数据可视化的4步<br>plt.<br><br>plt.<br><br>plt.<br><br>plt.<br><br>但是，x_fearures[:,1] 是什么意思呢？还有s=50又是指什么？？，为啥cmap=’viridis’？？？</p>
<h3 id="第三天"><a href="#第三天" class="headerlink" title="第三天"></a>第三天</h3><p>哈哈，我醒了，今天下午4点就醒了，比昨天早两个小时，昨晚上班不忙睡了一会今天早点起开心！<br></p>
<p>继续昨天的问题，s的意思，还有camp，必应了一下，发现简书的解释比较清晰，<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/53e49c02c469">lianjie</a> <br><br>但是，x_fearures两个数的意思还是想不明白，待我再看看。。</p>
<p>经过我测试
        <span class="lazyload-img-span">
        <img   
           data-src="https://github.com/Sncef/Sncef.github.io/blob/main/photo/1.jpg" >
        </sapn>
      <br>
        <span class="lazyload-img-span">
        <img   
           data-src="https://github.com/Sncef/Sncef.github.io/blob/main/photo/2.jpg" >
        </sapn>
      <br><br>
        <span class="lazyload-img-span">
        <img   
           data-src="https://github.com/Sncef/Sncef.github.io/blob/main/photo/3.jpg" >
        </sapn>
      <br>
        <span class="lazyload-img-span">
        <img   
           data-src="https://github.com/Sncef/Sncef.github.io/blob/main/photo/4.jpg" >
        </sapn>
      <br><br><br>我将问题指向了ARRAY这个数组，应该是数组的用法，后面我查询了数组，嘿嘿，我懂了<br>
        <span class="lazyload-img-span">
        <img   
           data-src="https://github.com/Sncef/Sncef.github.io/blob/main/photo/5.jpg" >
        </sapn>
      <br>python中数组和列表切片用法应该是相似而不同的，我用看列表切片的思维去看数组，导致我一直看不懂，哎。<br></p>
<p>终于可以接下来看了。。<br>可视化决策边界的前置知识<br><br>ylim(limits) 设置当前坐标轴或图表的 y 轴限制。将限制指定为窗体的两个元素向量 [ymin ymax], 其中 ymax 大于 ymin,<br><br>语法：X,Y = numpy.meshgrid(x, y)<br><br>输入的x，y，就是网格点的横纵坐标列向量（非矩阵）<br><br>输出的X，Y，就是坐标矩阵。<br><br><a target="_blank" rel="noopener" href="https://blog.csdn.net/lllxxq141592654/article/details/81532855">meshgrid</a><br><br>linspace是Matlab中的均分计算指令，用于产生x1,x2之间的N点行线性的矢量。<br><br><a target="_blank" rel="noopener" href="https://blog.csdn.net/anan15151529/article/details/102632463">predict_proba</a><br><br><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_28618765/article/details/78083895">reshape</a><br><br><a target="_blank" rel="noopener" href="https://numpy.org/doc/stable/reference/generated/numpy.ravel.html">ravel</a><br><br>可视化预测新样本的知识前置<br><br>用一个箭头指向要注释的地方，再写上一段话的行为，叫做annotate<br><br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/32501335">zhihu</a><br><br>训练样本，训练样本，我总觉得训练这两字怪怪的，哈哈<br><br><br><br>en,我觉得这个边界线跟结尾函数的对称点差不多嘛~<br></p>
<p>忽然对源代码有了点好奇，额，翻墙看了下是深渊，不止是坑那么简单的事了，先不管了,先掌握如何利用工具，而不探究工具本身，面向对象的好处不就是这样么，止住我的好奇心吧<a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/modules/classes.html">?</a><br><br>看到这里，我觉得这里的预测，指的是利用算法，把需要结果的概率求出，便是这里的含义了。<br><br>hhhh,到看花花的地方了，我也想一日看尽长安花的日子快点到来啊，要努力思考！<br><br>基于花花的知识前置<br><br>额，我觉得DSW里面讲的很详细了，每个都有备注哎，那么，这个画花花的人所要让我们知道的是啥呢？，不要只看别人让我看的，还要看他没有让我看的，才能收获更多<br><br>一开始是用简单的数据来画图，这个是用较复杂的互相有联系的数据来画图，是使用数据的手法吗，组合数据来进行可视化，有个有意思的事情，数据浅拷贝，防止对于原始数据的修改。那有深拷贝？？老板来两斤生蚝，我要5成熟！！！<br><br>我记得之前学列表引用也是一样的，新建一个列表，防止对原列表修改，是一样的手段。。<br></p>
<p>箱型图 三维散点图 <br></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ax.scatter(iris_all_class0[:,0], iris_all_class0[:,1], iris_all_class0[:,2],label&#x3D;&#39;setosa&#39;)</span><br><span class="line">ax.scatter(iris_all_class1[:,0], iris_all_class0[:,1], iris_all_class0[:,2],label&#x3D;&#39;versicolor&#39;)</span><br><span class="line">ax.scatter(iris_all_class2[:,0], iris_all_class0[:,1], iris_all_class0[:,2],label&#x3D;&#39;virginica&#39;)</span><br></pre></td></tr></table></figure>
<p>现在知道意思了，前天敲代码的时候有点懵逼<br></p>
<h4 id="Step-5"><a href="#Step-5" class="headerlink" title="Step 5"></a>Step 5</h4><p>我觉得这段话是重点，为了正确评估模型性能，将数据划分为训练集和测试集，并在训练集上训练模型，在测试集上验证模型性能。那么，如何实现呢，让我往下看。。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">iris_target &#x3D; data.target</span><br><span class="line">iris_features &#x3D; pd.DataFrame(data&#x3D;data.data, columns&#x3D;data.feature_names)</span><br><span class="line">iris_features_part &#x3D; iris_features.iloc[:100]</span><br><span class="line">iris_target_part &#x3D; iris_target[:100]</span><br></pre></td></tr></table></figure>
<p>为啥第四行没有.iloc呢？？？<br>明天继续吧，没时间了，明天休息时间多~~~ 干巴爹<br></p>
<h3 id="第四天"><a href="#第四天" class="headerlink" title="第四天"></a>第四天</h3><p>在群里看到了我最近隐隐感受到还没凝聚成语言的话语。<br><br>逻辑回归可以二分类，也可以多分类。他下面得到了三组参数，就证明做三分类的时候训练了三个分类器<br><br>所以数据的作用是得出预测未来的参数咯~<br><br>数据划分为两种，80%用于得出预测参数，20%用于验证准确性。<br><br>嗯，应该是有一个数据转化为参数的流程可以具体归纳的，我看看啊，利用model_selection划分数据，然后用了fit将数据转化为预测参数，<br>clf.fit，这个是用逻辑回归的算法clf，那么是不是有用其他算法就有其他xxx.fit呢，哈哈，如何查看逻辑回归得出的参数这步我知道就好了，黑匣子黑到底，<br>预测居然把80%也搞进去了，这个有必要？如果算法已经被验证是正常的那么这步应该不用吧，直接代入那20%不就可以了？？<br><br>在利用热力图对于结果进行可视化中，我没看到</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_predict &#x3D; clf.predict(x_train)</span><br><span class="line">print(&#39;The accuracy of the Logistic Regression is:&#39;, metrics.accuracy_score(y_train,train_predict))</span><br></pre></td></tr></table></figure>
<p>这两条代码对热力图的影响，是不是删除也不影响图呢？<br><br>另外我搜了搜[model_selection](<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_41861526/article/details/88617840?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-2.control&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-2.control%EF%BC%89">https://blog.csdn.net/qq_41861526/article/details/88617840?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-2.control&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-2.control）</a> 加深了model_selection的split的理解。<br><br>往上翻了下我才看出来二分和三分的代码区别，上面有个注释是这么说的，其对应的类别标签为，其中0，1，2分别代表<br>‘setosa’,’versicolor’,virginica’三种不同花的类别，每个类别数量那里也值得注意，2 50 1 50 0 50 ,也就解释了[:100]的意思了。<br><br>二分类那里用的是iris_features_part<br><br>三分类直接用iris_feature<br><br><a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E6%B7%B7%E6%B7%86%E7%9F%A9%E9%98%B5/10087822?fr=aladdin">混淆矩阵</a><br><br>通过结果我们发现·····出现了一定的错误。我们从可视化的时候也可以发现，这里指的是上面那三维三点图吧，蓝色的点附近毛都没有，绿色和橙色点<br>有几乎重合的的，也就是边界模糊。<br><br>##TASK 2<br>e，学代码先从敲代码开始</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">import warnings</span><br><span class="line">warnings.filterwarnings(&#39;ignore&#39;)</span><br><span class="line">import numpy as np</span><br><span class="line">from sklearn import datasets</span><br><span class="line">from sklearn.naive_bayes import GaussianNB</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line"></span><br><span class="line">X,y &#x3D; datasets.load_iris(return_X_y&#x3D;True)</span><br><span class="line">X_train, X_test, y_train, y_test &#x3D; train_test_split(X, y, test_size&#x3D;0.2, random_state&#x3D;0)</span><br><span class="line">clf &#x3D; GaussianNB(var_smoothing&#x3D;1e-8)</span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line">y_pred &#x3D; clf.predict(X_test)</span><br><span class="line">acc &#x3D; np.sum(y_test &#x3D;&#x3D; y_pred) &#x2F; X_test.shape[0]</span><br><span class="line">print(&#39;Test Acc : %.3f&#39;% acc)</span><br><span class="line"></span><br><span class="line">y_proba &#x3D; clf.predict_proba(X_test[:1])</span><br><span class="line">print(clf.predict(X_test[:1]))</span><br><span class="line">print(&#39;预计的概率值:&#39;, y_proba)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">import random</span><br><span class="line">import numpy as np </span><br><span class="line">from sklearn.naive_bayes import Categorica1NB</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line"></span><br><span class="line">rng &#x3D; np.random.RandomState(1)</span><br><span class="line">X &#x3D; rng.randint(5, size&#x3D;(600, 100))</span><br><span class="line">y &#x3D; np.array([1,2,3,4,5,6] * 100)</span><br><span class="line">data &#x3D; np.c_[X, y]</span><br><span class="line">random.shuffle(data)</span><br><span class="line">X &#x3D; data[:,:-1]</span><br><span class="line">y &#x3D; data[:, -1]</span><br><span class="line">X_train, X_test, y_train, y_test &#x3D; train_test_split(X, y , test_size&#x3D;0.2, random_state&#x3D;0)</span><br><span class="line"></span><br><span class="line">clf &#x3D; Categorica1NB(alpha&#x3D;1)</span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line">acc &#x3D; clf.score(X_test, y_test)</span><br><span class="line">print(&#39;Test Acc : %.3f % acc)</span><br><span class="line"></span><br><span class="line">x &#x3D; rng.randint(5, size&#x3D; (1, 100))</span><br><span class="line">print(clf.predict_proba(x))</span><br><span class="line">print(clf.predict(x))</span><br></pre></td></tr></table></figure>
<p>填个第三天结尾的疑问！<br><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/732858f89a00">jianshu</a><br>
        <span class="lazyload-img-span">
        <img   
           data-src="https://github.com/Sncef/Sncef.github.io/blob/main/photo/6.png" >
        </sapn>
      <br>
        <span class="lazyload-img-span">
        <img   
           data-src="https://github.com/Sncef/Sncef.github.io/blob/main/photo/7.png" >
        </sapn>
      <br>iloc对二维数据的读取用，第三行是表格，第四行是列表，不用iloc来选择。。</p>
<h3 id="第五天"><a href="#第五天" class="headerlink" title="第五天"></a>第五天</h3><p>TASK 2 先粗后细吧,还是先搜我不懂的内容<br><br><a target="_blank" rel="noopener" href="https://blog.konghy.cn/2017/12/16/python-warnings/">warnings</a><br><br><a target="_blank" rel="noopener" href="https://www.runoob.com/python/func-number-shuffle.html">shuffle</a><br></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X &#x3D; data[:,:-1]</span><br><span class="line">y &#x3D; data[:, -1]</span><br></pre></td></tr></table></figure>
<p>第二行少了个： 看不懂啊<br><br>需要计算的两个概率：条件概率，先验概率<br><br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/136791364">贝叶斯先验性</a><br><br>极大似然估计，只是一种概率论在统计学的应用，它是参数估计的方法之一。说的是已知某个随机样本满足某种概率分布，但是其中具体的参数不清楚，参数估计就是通过若干次试验，观察其结果，利用结果推出参数的大概值。极大似然估计是建立在这样的思想上：已知某个参数能使这个样本出现的概率最大，我们当然不会再去选择其他小概率的样本，所以干脆就把这个参数作为估计的真实值<br></p>
<p><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/9c153d82ba2d">https://www.jianshu.com/p/9c153d82ba2d</a><br></p>
<h6 id="我们可以看出就是对每一个变量的多加了一个频数alpha。当alpha-0时，就是极大似然估计。通常取值alpha-1，这就是拉普拉斯平滑，这又叫做贝叶斯估计，主要时因为如果使用极大似然估计，如果某个特征值在训练数据中没有出现，这时候会出现概率为0的情况，导致整个估计都为0，因此引入贝叶斯估计。"><a href="#我们可以看出就是对每一个变量的多加了一个频数alpha。当alpha-0时，就是极大似然估计。通常取值alpha-1，这就是拉普拉斯平滑，这又叫做贝叶斯估计，主要时因为如果使用极大似然估计，如果某个特征值在训练数据中没有出现，这时候会出现概率为0的情况，导致整个估计都为0，因此引入贝叶斯估计。" class="headerlink" title="我们可以看出就是对每一个变量的多加了一个频数alpha。当alpha=0时，就是极大似然估计。通常取值alpha=1，这就是拉普拉斯平滑，这又叫做贝叶斯估计，主要时因为如果使用极大似然估计，如果某个特征值在训练数据中没有出现，这时候会出现概率为0的情况，导致整个估计都为0，因此引入贝叶斯估计。"></a>我们可以看出就是对每一个变量的多加了一个频数alpha。当alpha=0时，就是极大似然估计。通常取值alpha=1，这就是拉普拉斯平滑，这又叫做贝叶斯估计，主要时因为如果使用极大似然估计，如果某个特征值在训练数据中没有出现，这时候会出现概率为0的情况，导致整个估计都为0，因此引入贝叶斯估计。</h6><p>学到现在。总感到有些地方不清晰，我想了想，是因为2.3里面的引入的数据集被隐藏了具体数据造成的，在TASK 1 中运用了数据查看的手段来看，这里没有，于是，我返回了TASK 1 重新学习查看数据的手段。<br></p>
<h6 id="好像先要将数据利用Pandas转化为DataFrame格式才能用下面的查看手段，我直接用报了个错AttributeError-‘numpy-ndarray’-object-has-no-attribute-‘head’-但是具体是不是真的还要实践。。。"><a href="#好像先要将数据利用Pandas转化为DataFrame格式才能用下面的查看手段，我直接用报了个错AttributeError-‘numpy-ndarray’-object-has-no-attribute-‘head’-但是具体是不是真的还要实践。。。" class="headerlink" title="好像先要将数据利用Pandas转化为DataFrame格式才能用下面的查看手段，我直接用报了个错AttributeError: ‘numpy.ndarray’ object has no attribute ‘head’,但是具体是不是真的还要实践。。。"></a>好像先要将数据利用Pandas转化为DataFrame格式才能用下面的查看手段，我直接用报了个错AttributeError: ‘numpy.ndarray’ object has no attribute ‘head’,但是具体是不是真的还要实践。。。</h6><p>TASK 1给出的方法有<br><br>利用.info()查看数据的整体信息 iris_features.head() <br><br>进行简单的数据查看，我们可以利用.head().tail() iris_features.head() <br><br>利用value_counts函数查看每个类别数量 pd.Series(iris_target).value_counts()<br><br>对于特征进行一些统计描述 iris_featues.describe() <br><br>额，今天就到这里了，留个悬念给明天。。剩下的一点时间看看能不能优化下空间哈！</p>
<h3 id="第六天"><a href="#第六天" class="headerlink" title="第六天"></a>第六天</h3><p>尝试掌握数据查看的方法。<br><br>额，不容易啊，一开始打多了一个字母，iris_features = pd.DataFrame(data=data.data, columns=data.feature_names)等于号左边的有s，右边的没有<br>。估计是一种命名习惯之类的，还在探索。学会了群里前辈给出的查看的方法：print(data.key()).<br><br>这时候，我忽然发现了————-</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(data.key())</span><br><span class="line">dict_keys([&#39;data&#39;, &#39;target&#39;, &#39;target_names&#39;, &#39;DESCR&#39;, &#39;feature_names&#39;])</span><br></pre></td></tr></table></figure>
<p>可是，我没设定target,DESCR啊，为啥会在keys里出现，莫非~~~~ 只能对这五个进行数据查看？为毛X_train看不了，然后，我仔细看了有关X_train诞生的代码，额，好像是随机数，可能，每次，不一样，所以，没法子看，只需要，知道，是，十分之八，就，好，了。。。看来，我还缺少很多能彻底看懂这三个task的前置知识啊，得多看书了。。。<br></p>
<h3 id="第7天"><a href="#第7天" class="headerlink" title="第7天"></a>第7天</h3><p>TASK 3<br><br>不知不觉时间已过半，第一天照常敲代码走起！！！<br></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">demoshujuji knn</span><br><span class="line"></span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pylot as plt</span><br><span class="line">from matplotlib.colors import ListedColormap</span><br><span class="line">from sklearn.neighbors import KNeighborsClassifier</span><br><span class="line">from sklearn import datasets</span><br><span class="line"></span><br><span class="line">iris &#x3D; datasets.load_iris()</span><br><span class="line">X &#x3D; iris.data[:, :2]</span><br><span class="line">y &#x3D; iris.target</span><br><span class="line"></span><br><span class="line">k_list &#x3D; [1, 3, 5, 8, 10, 15]</span><br><span class="line">h &#x3D; .02</span><br><span class="line">camp_light &#x3D; ListedColormap([&#39;orange&#39;, &#39;cyan&#39;, &#39;cornflowerblue&#39;])</span><br><span class="line">camp_bold &#x3D; ListedColormap([&#39;darkorange&#39;, &#39;c&#39;, &#39;darkblue&#39;])</span><br><span class="line"></span><br><span class="line">plt.figure(ifgsize&#x3D;(15,14))</span><br><span class="line"></span><br><span class="line">for ind,k in enumerate(k_list):</span><br><span class="line">    clf &#x3D; KNeighborsClassifier(k):</span><br><span class="line">    clf.fit(X, y)</span><br><span class="line">    </span><br><span class="line">    x_min, x_max &#x3D; X[:, 0].min() - 1, X[:, 0].max() +1</span><br><span class="line">    y_min, y_max &#x3D; X[:, 0].min() - 1, X[:, 0].max() +1</span><br><span class="line">    xx, yy &#x3D; np.meshgrid(np.arrange(x_min, x_max, h),</span><br><span class="line">                        (np.arrange(y_min, y_max, h))</span><br><span class="line">    z &#x3D; clf.predict(np.c_[xx.ravel(), yy.ravel()])</span><br><span class="line">    z &#x3D; Z.reshape(xx.shape)</span><br><span class="line">    </span><br><span class="line">    plt.subplot(321+ind)</span><br><span class="line">    plt.pcolormesh(xx, yy, Z, cmpa&#x3D;camp_light)</span><br><span class="line">    plt.scatter(X[:, 0], X[:, 1], c&#x3D; y, cmap&#x3D;cmap_bold.</span><br><span class="line">                edgecolor&#x3D;&#39;k&#39;, s&#x3D;20)</span><br><span class="line">    plt.xlim(xx.min(), xx.max())</span><br><span class="line">    plt.ylim(yy.min(), yy.max())</span><br><span class="line">    plt.title(&quot;3-Class classification (k &#x3D; %i)&quot;% k)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">from sklearn import datasets</span><br><span class="line">from sklearn.neighbors import KNeighborsClassifier</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line"></span><br><span class="line">iris &#x3D; dataset.load_iris()</span><br><span class="line">X &#x3D; iris.data</span><br><span class="line">y &#x3D; iris.target</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test &#x3D; train_test_split(X, y, test_size&#x3D;0.2)</span><br><span class="line"></span><br><span class="line">clf &#x3D; KNeighborsClassifier(n_neighbors&#x3D;5, p&#x3D;2, metric&#x3D;&quot;minkowski&quot;)</span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">X_pred &#x3D; clf.predict(X_test)</span><br><span class="line">acc &#x3D; sum(X_pred &#x3D;&#x3D; y_test) &#x2F; X_pred.shape[0]</span><br><span class="line">print(&quot;预测的准确率ACC： %.3f&quot; % acc)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt </span><br><span class="line">from sklearn.neighbors import KNeighborsRegressor</span><br><span class="line">np.random.seed(0)</span><br><span class="line">X &#x3D; np.sort(5*np.random.rand(40, 1), axis&#x3D;0)</span><br><span class="line">T &#x3D; np.linspace(0, 5, 500)[:, np.newaxis]</span><br><span class="line">y &#x3D; np.sin(X).ravel()</span><br><span class="line">y[::5] +&#x3D; 1*(0.5 - np.random.rand(8))</span><br><span class="line"></span><br><span class="line">n_neighbors &#x3D; [1, 3, 5, 8, 10, 40]</span><br><span class="line">plt.figure(figsize&#x3D;(10,20))</span><br><span class="line">for i,k in enumerate(n_neighbors):</span><br><span class="line">    clf &#x3D; KNeighborsRegressor(n_neighbors&#x3D;k, p&#x3D;2, mietric&#x3D;&quot;minkowski&quot;)</span><br><span class="line">    clf.fit(X, y)</span><br><span class="line">    y_ &#x3D; clf.predict(T)</span><br><span class="line">    plt.subplot(6, 1, i + 1)</span><br><span class="line">    plt.scatter(X, y, color&#x3D;&#39;red&#39;, label&#x3D;&#39;data&#39;)</span><br><span class="line">    plt.plot(T, y_, color&#x3D;&#39;navy&#39;, label&#x3D;&#39;prediction&#39;)</span><br><span class="line">    plt.axis(&#39;tight&#39;)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.title(&quot;KNeighborsRegressor (k &#x3D; %i)&quot; % (k))</span><br><span class="line">plt. tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">!wget https:&#x2F;&#x2F;tianchi-media.oss-cn-beijing.aliyuncs.com&#x2F;DSW&#x2F;3K&#x2F;horse-colic.csv</span><br><span class="line">!wget https:&#x2F;&#x2F;tianchi-media.oss-cn-beijing.aliyuncs.com&#x2F;DSW&#x2F;3K&#x2F;horse-colic.names</span><br><span class="line"></span><br><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">from sklearn.neighbors import KNeighborsClassifier</span><br><span class="line">from sklearn.impute import KNNImputer</span><br><span class="line">from sklearn.metrics.pairwise import nan_euclidean_distances</span><br><span class="line">from sklearn.model_selection import cross_val_score</span><br><span class="line">from sklearn.model_selection import RepeatedStratifiedKFold</span><br><span class="line">from sklearn.pipline import Pipeline</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line"></span><br><span class="line">X &#x3D; [[1, 2 np.nan], [3, 4, 3], [np.nan, 6, 5], [8, 8, 7]]</span><br><span class="line">imputer &#x3D; KNNImputer(n_neighbors&#x3D;2, metric&#x3D;&#39;nan_euclidean&#39;)</span><br><span class="line">imputer.fit_transform(X)</span><br><span class="line"></span><br><span class="line">nan_edclidean_distances([[np.nan, 6, 5], [3, 4, 3]], [[3, 4, 3], [1, 2, np.nan], [8, 8, 7]])</span><br><span class="line"></span><br><span class="line">input_file &#x3D; &#39;.&#x2F;horse-colic.csv&#39;</span><br><span class="line">df_data &#x3D; pd.read_csv(input_file, header&#x3D; None, na_values&#x3D;&#39;?&#39;)</span><br><span class="line">data &#x3D; df_data.values</span><br><span class="line">ix &#x3D; [i for i in range(data.shape[1]) if i !&#x3D; 23]</span><br><span class="line">X, y &#x3D; adta[:, ix], data[:, 23&#39;</span><br><span class="line"></span><br><span class="line">for i in range(df_data.shape[1]):</span><br><span class="line">    n_miss df_data[[i]].isnull().sum()</span><br><span class="line">    perc &#x3D; n_miss &#x2F; df_data,shape[0] *100</span><br><span class="line">    if n_miss.values[0] &gt; 0:</span><br><span class="line">        print(&#39;&gt;Feat: %d, Missing: %d, Missing ratio: (%.2f%%)&#39; % (i, n_miss, perc))</span><br><span class="line"></span><br><span class="line">print(&#39;KNNImputer before Missing: %d&#39; % sum(np.isnan(X).flatten()))</span><br><span class="line">imputer &#x3D; KNNImputer()</span><br><span class="line">imputer.fit(X)</span><br><span class="line">Xtrans &#x3D; imputer.transform(X)</span><br><span class="line">print(&#39;KNNImputer after Missing: %d&#39; % sum(np.isnan(Xtrans).flatten()))</span><br><span class="line"></span><br><span class="line">results &#x3D; list()</span><br><span class="line">strategis &#x3D; [str(i) for i in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10 ,15, 16, 20, 21]]</span><br><span class="line">for s in strategies:</span><br><span class="line">pipe &#x3D; Pipeline(steps&#x3D;[(&#39;imputer&#39;, KNNImputer(n_neighbors&#x3D;int(s))), (&#39;model&#39;, KNeighborsClassifier())])</span><br><span class="line">scores &#x3D; []</span><br><span class="line">for k in range(20):</span><br><span class="line">    X_train, X_test, y_train, y_test &#x3D; train_test_split(Xtrans, y, test_size&#x3D;0.2)</span><br><span class="line">    pipe.fit(X_train, y_train)</span><br><span class="line">    score &#x3D; pipe.score(X_test, y_test)</span><br><span class="line">    scores.append(score)</span><br><span class="line">results.append(np.array(scores))</span><br><span class="line">print(&#39;&gt;k: %s, Acc Mean: %.3f, Std: %.3f&#39; % (s, np.mean(scores), np.std(scores)))</span><br><span class="line">boxplot(results, labels&#x3D; strategies. showmeans&#x3D;True)</span><br><span class="line">show()</span><br></pre></td></tr></table></figure>
<h3 id="第八天"><a href="#第八天" class="headerlink" title="第八天"></a>第八天</h3><p>将我不懂的列出来<br><br><a target="_blank" rel="noopener" href="https://www.runoob.com/python/python-func-enumerate.html">enumerate</a> 可是前面的ind,k什么意思？我还想不明白。。<br><br><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/de223a79217a">subplot</a> 看到这个，我想IND应该是跟辅助排序有关的吧。。。<br><br>根据6张图的变化，可以看出画布颜色的变化与K值有关，但是是怎么影响的，为啥较小的k值就相当于用较小的领域中的训练实例进行预测？好想把函数中的代码挖出来看。<br><br><a target="_blank" rel="noopener" href="https://www.cnblogs.com/lijunjie9502/p/10327151.html">figsize</a><br><br>hhh,好像找到了！！！<a target="_blank" rel="noopener" href="https://www.cnblogs.com/OliverQin/p/7965435.html">edgecolor</a> 这里的K等价于edgecolor，这个值越大，边界轮廓越光滑！！！！，然后作者借用这特性，来进行K分类。。<br></p>
<h3 id="第九天"><a href="#第九天" class="headerlink" title="第九天"></a>第九天</h3><p>欧式距离：minkowski <br><br>定义CLF的算法，FIT数据。。<br><br>用生成的FIT预测X_pred，ACC的计算<br><br>这里的KNN.FIT(X,y)是指n_neighbors=5, p=2 ??? label对应P=2，分为0，1，2三部分，这里要注意从0开始数。。设最小的为0，编号1&lt;br?<br>k在表格的圈定<br><br>拟合，欠拟合。K值的选择最终要与训练数据相适应，才是好的K <br><br>最后的有点复杂，明天看吧<br></p>
<h3 id="第十天"><a href="#第十天" class="headerlink" title="第十天"></a>第十天</h3><p>总体来看，就是数据跟现实更接近了一点，用不完整的数据进行KNN分类，那如何对不完整的数据进行处理，然后得到好的KNN分类效果，就是2.4.4的主体内容。<br><br>KNNImputer空值填充原理：计算每个样本最近的K的样本，进行空值填充。<br><br>重点句子：上例中，我们看到是一个3维，只有第二维全部非空，所以只能计算第二维，但是第一维和第三维是不能忽略的，所以将第一维和<br>第三维的计算加到第二维上，所以要乘以三。当然原文不是这样的，但是原文的意思就是这样！！！<br><br>欧式距离是用来得出最近样本的，用来排序的！！！！<br><br>只有排完序才能用加权平均计算空值<br><br>pipline,数据管道，任何有序的操作都可以看做pipline</p>
<h3 id="昨天看在魔王城说晚安了，今天继续。。"><a href="#昨天看在魔王城说晚安了，今天继续。。" class="headerlink" title="昨天看在魔王城说晚安了，今天继续。。"></a>昨天看在魔王城说晚安了，今天继续。。</h3><p>下载了幸福感的资料，看了下内容。按我如今的水平，想要建模计算出来，额短时间我是不可能的了。只能想个实现的思路。首先，数据给出的幸福感资料里有众多相关联的变量，但是，或许是错的呢，有些变量或许与<br>幸福感根本没有任何关联，可能与幸福感有关联的变量没有写入，因此，数据的进一步选择我认为是很重要的，只有把握到切实影响的数据，才能减少误差增加速度，而且，每种变量对应的无效数据不一定是相同的，有<br>些无效数据是负数，有些无效数据是数据调查人为了隐藏真实而提供的虚假数据，这些数据与幸福感亦无关联，应通过算法将其识别剔除。如何将这些不同类型的数据与幸福感数据牵引在一起，我觉得，可以用赋权重的<br>办法来建立一条公式，公式左边是幸福感，右边是不同类型的数据，按照权重将各种数据给分，例如收入和精神境界分给多一点，一些次要的分给少点，分别计算，然后得出幸福感的分。。。但是这种算法我想准确率是<br>不够高的，不可能我几分钟内就能想到一个完美的方法，方法是逐渐完美的，这种方法就不适用于流落街头却精神富足的人，因为他即使身无分文但却拥抱幸福，算法应考虑这种小小的情况，把大部分情况都考虑，才能<br>得到较好的算法，但是完美的算法是不可能的，因为知觉的有限性决定了人的局限性，总会有一些没有考虑到的情况。</p>

        <!-- 分类文章 -->
        
      </div>
      <div class="post-content-inner-space">
        
          <div class="space-toc-main animate__animated  animate__fadeInUp">
            <ol class="space-toc"><li class="space-toc-item space-toc-level-2"><a class="space-toc-link" href="#MY-amp-PY%E5%A4%A9%E6%B1%A0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0"><span class="space-toc-text">MY&amp;PY天池机器学习</span></a></li></ol>
           </div>
        
      </div>
   </div>
    <!-- 评论 -->
    
  </div>
</article>
  </div>
</div>



<!-- 如果是home模式的话，不在首页就显示footer，如果不是home模式的话 所有都显示footer -->

  <div class="footer-outer animate__animated  animate__fadeInUp">
    <div class="footer-inner">
    <div class="footer-text">
    <p>Power by <a target="_blank" rel="noopener" href="http://hexo.io/">Hexo</a> Theme by <a href="https://github.com/FuShaoLei/hexo-theme-white">White</a></p>

    </div>
    <div class="footer-contact">
    <ul class="footer-ul">
        
        <li class="footer-li">
            <a href="https://github.com/Sncef/Sncef.github.io" target="_blank">
                <i class="ri-github-line"></i>
            </a>
        </li>
        
        <li class="footer-li">
            <a href="mailto:321526645@qq.com" target="_blank">
                <i class="ri-mail-line"></i>
            </a>
        </li>
        
    </ul>
    </div>
    </div>
</div>






<script src="/js/white.js"></script>



</body>
</html>
